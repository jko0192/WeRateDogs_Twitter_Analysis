{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, the data wrangling process involved gathering three sources of data, assessing them for tidiness and data quality issues, and cleaning those issues to create a clean master dataset for analysis and exploration. \n",
    "\n",
    "<b>Gathering Data</b>\n",
    "\n",
    "The three sources of data gathered were a twitter archive csv file, an image predictions tsv file containing data pulled programmatically from udacity's server, and a json text file created from json data pulled from twitter's api. The image predictions tsv file was created from data pulled from udacity's server using the requests library and storing the response content into the file. The json text file was created from json data pulled from twitter's api which required consumer key, consumer secret, access token, and access secret for credentials.\n",
    "\n",
    "<b>Assessing Data</b>\n",
    "\n",
    "Assessment for these data sets involved documenting and identifying tidiness and data quality issues. First visual assessments on these data sets helped me get a strong understanding of what data variables I will be working with. Afterwards, programmatic assessments helped identify the tidiness and data quality issues more clearly than visual assessments. Several tidiness issues included having two separate related data sets, having each stage variable in its own column, and having source embedded in html instead of its own column. Several data quality issues included rows with retweets, rows without images, incorrect dog names extracted from texts, incorrect ratings extracted from texts, and data types issues. There were missing values for several variables documented in the assessment stage \n",
    "\n",
    "<b>Cleaning Data</b>\n",
    "\n",
    "Cleaning these issues found in the assessment stage was an organized process. For each issue, the cleaning process involved defining the issue, cleaning the issue with code, and testing the code. First the tidiness issues were fixed, and then data quality issues were fixed. Pandas, numpy, and regex functions helped clean these issues effectively and easily. I used for loops to reduce repetition and filtering for subsets of data to help resolve the issue easier. Once these issues were fixed, a clean master data set was produced for analysis, exploration, and visualization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
